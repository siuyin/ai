# Notes on Artificial Intelligence

Loh Siu Yin
Developer, Beyond Broadcast LLP
siuyin@beyondbroadcast.com
19 Apr 2020


These notes are based on "CS50: Introduction to AI with Python by Harvard University".
That course was first published on EdX on 1 April 2020 and delivered by Brian Yu.
brian@cs.harvard.edu

While the course language was `python`, I re-implemented key concepts with `Go`
to reinforce my understanding of the course content.


## Search
Given a problem with a defined starting point,  
find a series of actions
that will lead to a, preferably optimal, solution.

Let S be the state of the world, and  
A be the set of actions when applied to the world  
results in a new state S'.

The transition from S to S' can then be modelled as a function:  
`T(S,A) -> S'.`

A is a function of the current state, S.
Thus A should be written: A(S).

In order to determine if a solution has been found,  
we define the goal state, G.  
If S = G, then the state of the world is at _a_ goal state
(there may be many states S that satisfy goal state G).  

The path from S to G can be tracked,
if each S maintains a pointer to its parent state Sprev.

### Go Implementation with Breadth-First Search

Let us invoke the search thus:

.code search/search_test.go /invS/,/invE/ HL01

`transitionModel` and `availableActions` are defined in the code
calling `Search`.
I will expand on their implementation later.

`Search` searches breadth-first:

.code search/search.go  /schS/,/schE/ HL01

`breadthFirstSearch.search` is listed below.
This implementation was guided by [pseudocode](https://en.wikipedia.org/wiki/Breadth-first_search#Pseudocode)
given in wikipedia.

.code search/search.go /bfsS/,/bfsE/

#### Transition Model and Available Actions

The transition model T, returns the next state S', when action A acts on current state S.  
i.e. T(S,A) -> S'

.code search/search_test.go /tmS/,/tmE/

`availableActions` implements A(S) and returns a list of actions:

.code search/search_test.go /aaS/,/aaE/

#### Example run

Example code:

.code search/search_test.go /invS/,/invE/ HL01

Each `State` is printed out thus:

.code search/search.go /strS/,/strE/

Test run:

```
siuyin@godev:~/go/github.com/siuyin/ai/search$ go test
--- FAIL: ExampleSearch (0.00s)
got:
[(4: <--) (5: <--) (6: <--) (7: <--) (8: <--) (9: <--) (10: <--) (11: <--) (12: )]
want:
Should fail to demonstrate output
FAIL
```

### Go Implementation with Depth-First Search

Depth first search essentially uses the same algorithm as breadth first search.
However a stack is used instead of a queue.

.code search/search.go /dfsS/,/dfsE/

The differences with breadth first search are highlighted in the code below:

.code search/search.go /dfmS/,/dfmE/


### Other search algorithms

These include:
1. A-star,  a heuristic enhanced search.
1. Iterative deepening depth-first search (only for directed graphs).

## Storing and Using Knowledge in computers
How do we:
1. Store knowledge in a machine-readable form?
1. Use that stored knowledge to make inferences?

### Storage
Use a set of 'sentences'.

Q: set or sequence? Is there a temporal dependency?
Is the knowledge valid all the time or just some of the time?
For knowledge to be useful, all we need it that it is valid at the time of use.
Thus we can store knowledge as a set of sentences.

A lot has been written about the subject. See [the wikipedia entry](https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning) .

Each sentence has a context-free grammar of terms.



## Representing Knowledge

Given the knowledge an AI has, what new inferences or deductions (entailments)
can our AI make?

Given the knowledge:  
A: temperature is 100 deg Celcius  
B: water is boiling  
C: atmospheric pressure is 1 Bar

We have 3 separate statements of fact.

Can a computer program infer `A` when `B` and `C` are observed to be true?

In code: `if B and C then A` or ` if A and B then C` or `if A and C then B`.

We started with 3 statements or `propositions` and through `inference` we can
add 3 more statements.

To do this we have to represent knowledge in a form our AI can understand.

Two ways to represent knowledge are with:
1. Propositional logic
1. First-order logic

### Propositional logic
Propositional logic has a language comprising
sentences, symbols, logical connectives, models and knowledge bases.

#### Sentences, Symbols and Logical connectives

A `sentence` is an assertion about the world in
a knowledge representation language.

The `sentence` is represented by a `symbol` like `P`.

`Logical connectives` are symbols like `^ v ¬ ->` and `<->`.  
The above mean: `and or not implies` and `biconditional` respectively.

##### Examples of sentences

1. It is raining. (S1)
1. It is sunny. (S2)
1. S1 and S2 : It is both raining and sunny.
1. S1 or S2 : It is either raining or sunny.
1. **¬**S1 : It is not raining. Also !S1 or not S1.
1. There are clouds. (S3)
1. S1 **->** S3. Reads S1 implies S3. **If** it is raining, **then** there are clouds.
1. Water droplets are falling from the sky. (S4)
1. S1 **<->** S4. Reads it is raining **if and only if** water droplets 
  are falling from the sky.
  Conversely water droplets are falling from the sky **if and only if** it is raining.

S1 through S4 are `symbols` representing the 'original', 'primitive' or 'atomic' `sentences`.

`Logical connectives` augment `symbols` to form new 'compound' `sentences`.

##### Go implementation of sentences and symbols

A `sentence` in `go` is a `type` with 3 methods.

.code know/know.go /senS/,/senE/

In CS50, the sentence class was used as the base class for
derived classes like `symbol`, `and`, `or`, `inference` and
`biconditional`.

Go does not support class inheritance but does support interface.
Hence `symbol` etc are implemented as `types` which implicitly
implement the `Prop` interface.

.code know/know.go /proS/,/proE/

The `Evaluate` method evaluates against a model and returns `true`
or `false`.

In Go, `model` is a set of symbols. It is implemented as
a map of `Prop` to `bool`.

.code know/know.go /type symbolSet/,/$/

The test code below shows how symbol, p, evaluates against  model, m.
A symbol is a Proposition. That symbol could be true or false.
A model is a set of symbols with truth values assigned to each symbol.

In the code example below `true` is assigned to symbol `P`.

.code know/know_test.go /func TestModel/,/^}$/

Note the lower-cased `symbol` is private / unexported and not accessible from outside
the package.  
External code access `symbol` via the `Sym` function.

.code know/know.go /func Sym/,/^}$/

Connectives like `And` and `Or` are similarly exported.
`And()` takes zero or more `Prop`s. And() works as a
constructor to return an `and` Prop.

.code know/know.go /func And/,/^}$/
.code know/know.go /type and/,/^}$/

#### Models and Knowledge Bases
A `model` is the assignment of a truth value
to every propositional symbol.
Thus a `model` represents a possible world.

If there `n` propositional symbols, then there are `2^n` possible models.

A `knowledge base` is a set of sentences known by the
knowledge-based agent.  
They are things about the world the AI knows.

##### Go example on use of model checking and entailment

In the test code below, our knowledge base kb is "`p and q`"
or "`p ^ q`".
We want to check if the knowledge `p ^ q` entails `q`.
In other words if we know `p ^ q` to be true, can the code
infer `q` is also true?

We know from mathematics that this is true, however the
`model check` code can do this  algorithmically.

.code know/know_test.go /func TestModelCheck/,/^}$/

#### Entailment ⊨ 

`α ⊨ β` : alpha `entails` beta if,  
in every model in which `sentence` alpha is true,  
`sentence` beta is also true.  

Thus if in every `model`, I find `alpha` to be true and
`beta` is also true,  
then I can conclude that `alpha entails beta`.

Let `A` and `B` be `symbols`, then  
If `A entails B`  
I can add to the knowledge base the new sentence: `A -> B ( A infers B )`.

#### Inference
Given a knowledge base, what entailments can an AI `infer`?

`Inference` is the process of deriving new knowledge from old knowledge.

If `alpha` is sentence posed to the AI,  
We want to know if `knowledge base entails alpha`.

An algorithm that takes the knowledge base as _input_ and _outputs_
if knowledge base entails alpha is **model checking**

#### Model Checking

To determine if `knowledge base entails alpha`:
1. Enumerate all possible models
1. If in every model,  
   knowledge base is true and  
   alpha is true, then  
   knowledge base entails alpha.
   
##### Go application using model checking

The knowledge representation package is imported as `k` for brevity:

.code know/cmd/knights-knave/main.go /k "github.com/,/$/
.code know/cmd/knights-knave/main.go /func main/,/^}/

`knowledgeAbout` defines the knowledge base.
Notice here I've used the convenience method Add to add propositions to
the knowledge base.

.code know/cmd/knights-knave/main.go /func knowledgeAbout/,/^}/


`findEntailments` uses a model checking algorithm to make inferences.
The `qry :=range kb.Symbols()` loop, model checks kb to
determine if kb entails any of its symbols: kni or kna.

.code know/cmd/knights-knave/main.go /func findEntailments/,/^}/

### First-order logic

Propositional logic suffers from combinatorial explosion. Consider 4 sports
houses in a School. Further consider 4 students who each belong to a different house.

Let H1 to H4 be the 4 houses and S1 and S4 be the 4 students. In propositional logic
we will have symbols H1S1 H1S2 ... H4S3 H4S4, giving a total of 16 symbols.

∀ Student, Student has ID. Reads for all students, each student has an ID.

Ǝ Student, Student has 4.0 GPA. Reads there exists some students, where the student has a 4.0 grade point average.

## Uncertainty

A 'random variable' in probability theory is a variable that can take
on values drawn from its domain of values.

Eg. 'roll' could be a random variable for rolls of a dice. The values
'roll' can take on range from 1 through 6.

A probability distribution over a random variable X is a list. The value of each element in the
list is the probability that X takes on a given value from its domain of values.

Eg. The probability distribution 'roll' for a fair dice is illustrated below:
(1,1/6), (2,1/6), ... (6,1/6)

The first value of the tuple is the value of 'roll'. The second value, p, of the
tuple is the probability of that 'roll' having that value p.  
Hence the probability distribution over 'roll' has six elements: [1/6,1/6, ..., 1/6].

### Joint probability distribution
A joint probability distribution is the probability distribution over two or more
random variables.

Example. Let PD(D) be the probability distribution over random variable D representing dice rolls.
And PD(C) be the probability distribution over random variable C representing coin tosses.
Then PD(D,C) is a list of 6*2 = 12 values. [1/6 * 1/2, 1/6 * 1/2, ... , 1/6 * 1/2].

### Useful Probability Formulae

#### Bayes' rule

.html math/bayes.html 

Bayes' rule is usually stated in the form of Equ. 2.

#### Marginalization
A technique to derive unconditional probabilities from joint probabilities.

.html math/margin.html

#### Conditioning
Very much related to marginalization:

.html math/conditioning.html

## Bayesian Networks

A Bayesian network is a directed acyclic graph.

The edge (A,B) represents the temporal relationship: A was presented
before B.

Nodes hold probability distribution over a random variable.
The value of the node B, is PD(B|A), where PD means "probability distribution over".

The network A -> B reads as follows:
1. Node A holds the unconditional probability distribution of A occurring.
1. Node B holds the probability distribution of B occurring, given A has occurred.






.html math/mathjax.html
